就把错推给⼈类吧
我们已经到了⼀个历史转折点：当下历史的重⼤进程，有⼀部分是由
⾮⼈类智能的决定推动的。正因如此，计算机⽹络的易错性才变得如
此危险。计算机犯错原本不算什么，但当计算机成了历史推动者时，
这些错误就可能带来灾难。第六章已经提及这个论点，当时曾简单提
及脸书在煽动罗兴亚⼈冲突中扮演的⻆⾊。但也如当时所⾔，许多⼈
（包括脸书、YouTube与其他科技巨头的部分⾼管与⼯程师）对这个
论点并不赞同。但由于这是本书的⼀个核⼼论点，因此我们最好再深
⼊研究⼀下，详细地谈谈各⽅反对这个论点的原因。
脸书、YouTube、TikTok等平台的管理者为了找借⼝，常常会说这些
问题不是算法带来的，⽽是源于⼈性。他们表示，⼈性是各个平台上
⼀切仇恨与谎⾔的源头。这些科技巨头认为，因为他们信守⾔论⾃由
的价值观，所以审查⼈们表达真情实感真的很难。例如，YouTube原
⾸席执⾏官苏珊·沃⻄基在2019年解释说：“我们对这件事的思考⽅式
是：‘这个内容有没有违反我们的某项政策？是否违反了对于仇恨、骚
扰等⽅⾯的规定？’如果它确实违反了，我们就会将其删除。其实我们
的政策越来越严。当然，我们也会受到批评。⾔论⾃由的界限到底应
该划在哪⾥？要是划得太严，是不是就会抹除社会应该听到的某些声
⾳？”
脸书⼀位发⾔⼈在2021年10⽉表示：“正如所有其他平台，我们⼀直
得在⾔论⾃由与有害⾔论、安全与其他议题之间做出艰难决定……但
像这些社会界限，最好都交给⺠选的领导者来判断。”于是，科技巨
头不断转移讨论焦点，说⾃⼰只是在⼈类⽤户制作出内容之后，担任
版主这样的⻆⾊，很⾟苦，⽽且⼤多数时候发挥的是积极作⽤。这让
⼈觉得似乎问题都是⼈类⾃⼰造成的，算法在尽⼒限制⼈性恶的⼀
⾯。他们却绝⼝不提⾃⼰的算法会主动助⻓某些⼈类情绪，同时抑制
另⼀些⼈类情绪。难道他们真的没看到这些情形吗？
当然不是。早在2016年，脸书的⼀份内部报告就发现“在加⼊极端组
织的⼈当中，有64%是因为我们的推荐⼯具才加⼊极端组织的……我
们的推荐系统助⻓了这个问题”。“吹哨⼈”脸书前员⼯弗朗⻄丝·豪根
揭露了脸书在2019年8⽉的⼀份内部机密备忘录，⾥⾯提及：“我们有
许多不同来源的证据显示，脸书及其‘应⽤程序家族’上的仇恨⾔论，
以及分裂性政治⾔论与错误信息，正在影响世界各地的社会。我们也
有令⼈信服的证据指出，我们的核⼼产品机制，如病毒式传播、推荐
与提升⽤户黏性，正是此类⾔论在脸书平台盛⾏的重要原因。”
2019年12⽉流出的另⼀份⽂件则提及：“病毒式传播不同于我们和好
友或家⼈的沟通，⽽是⼀种由我们带到许多⽣态系统之中的新事
物……这种事之所以发⽣，是因为我们出于商业⽬的⽽刻意推动
的。”⽂件指出：“以参与度为标准对有较⾼⻛险的话题（如健康或政
治）进⾏排序，会导致不正当激励与诚信问题。”它还提及，或许最
致命的⼀点在于“我们的排序系统包括各种具体的独⽴预测，不仅预
测你可能参与哪些内容，还预测你可能传播哪些内容。遗憾的是，研
究显示，让⼈感到愤慨的内容与错误信息更有可能像病毒那样传
播”。这份⽂件提出⼀条重要建议：由于平台⽤户多达数百万⼈，脸
书不可能删除所有有害内容，但⾄少应该“别再让有害内容得到⾮⾃
然传播⽽得到放⼤”。
正如⼀些国家的领导⼈，科技企业正在做的并不是找出关于⼈类的真
相，⽽是给⼈类强加⼀个扭曲的新秩序。⼈类是⾮常复杂的⽣物，良
好社会秩序会培养⼈类的美德，同时减少⼈类的消极倾向。然⽽社交
媒体算法只把⼈看成矿井，想要“开采”更多的注意⼒。⼈类有丰富的
情感，如爱、恨、愤慨、喜悦、困惑，但算法把⼀切简化成⼀件事
——参与度。⽆论是2016年在缅甸、2018年在巴⻄，还是在许多其他
国家，算法在对所有视频、帖⽂或其他内容进⾏评分时，只看⼈们观
看了⼏分钟、分享了⼏次。能够让⼈看⼀⼩时的谎⾔或仇恨内容的评
分就是⾼于只让⼈看10分钟的有关真相或令⼈产⽣同情⼼的内容，前
者甚⾄⽐睡⼀⼩时还重要。即使事实摆在眼前，即谎⾔与仇恨常常会
对⼼理与社会造成破坏，⽽真相、同情⼼与睡眠是⼈类幸福所不可或
缺的，算法也完全不考量这⼀点。正因为对⼈类的理解如此狭隘，所
以算法创造了⼀个新的社会系统，⿎励我们顺从⼈类最基本的本能，
同时阻碍我们发挥⼈类完整的潜能。
随着各种不良影响的⽇益显现，科技巨头也不断被警告要注意这些影
响，但因为它们坚信天真的信息观，所以未能进⾏⼲预。明明平台上
充斥着各种谎⾔与骇⼈听闻的内容，但企业⾼层还是认为只要让更多
的⼈⾃由表达想法，真理就会占据上⻛。但事态并未如此发展。我们
⼀次⼜⼀次看到，如果⼈们可以完全⾃由地表达⾃⼰，真理常常会败
下阵来。想让天平往有利于真理的⽅向倾斜，⽹络就必须发展并维持
强⼤的⾃我修正机制，让说真话的⼈得到奖励。⾃我修正机制需要付
出相当⼤的代价，但要是真想得到真理，这是必要的代价。
硅⾕以为⾃⼰不需要在意这条历史规律，⽽各个社交媒体平台也⼀直
缺乏⾃我修正机制。2014年，脸书只有⼀位缅甸语内容管理者，负责
监控整个缅甸的活动内容。有些⼈已经注意到了缅甸的状况，警告脸
书必须加强内容审核，但脸书不以为意。⽐如出身于缅甸农村的缅裔
美籍⼯程师暨电信业⾼层佩因特·吞就曾多次致信脸书⾼层警惕这种危
险。早在2014年7⽉5⽇，距离种族冲突还有两年，她就在⼀封电⼦邮
件中提出预⾔般的警告：“令⼈痛⼼的是，脸书⽬前在缅甸被使⽤的
⽅式，就跟⼴播在卢旺达种族灭绝的那个⿊暗时期被使⽤的⽅式⼀
样。”⽽脸书却没有采取任何⾏动。
即使对罗兴亚⼈的攻击不断升温，脸书⾯临的批评排⼭倒海，它也拒
绝聘请真正了解当地情况的⼈管理内容。因此，在得知有⼀群缅甸仇
恨分⼦使⽤缅甸语“kalar”⼀词作为对罗兴亚⼈的种族歧视称呼之后，
脸书在2017年4⽉的反应是完全禁⽌在脸书平台发布使⽤这个词的内
容。但这暴露了脸书完全不了解当地情况与缅甸语。在缅甸语中，
kalar只有在特定语境下才是⼀种种族歧视称呼，⽽在其他语境下则完
全与种族歧视⽆关。⽐如缅甸语中“椅⼦”是kalar htaing，“鹰嘴⾖”则
是kalar pae。佩因特·吞在2017年6⽉写给脸书的邮件⾥就指出，禁⽌
在脸书发⽂时使⽤kalar这个词，就像是禁⽌在写“hello”时写
出“hell”（地狱）⼀样。但脸书还是继续⽆视⾃⼰对当地专业⼈⼠的需
求。截⾄2018年4⽉，在缅甸的脸书⽤户数⾼达1800万，⽽脸书雇⽤的
懂缅甸语的内容审核⼈员只有5⼈。
各个社交媒体巨头所做的，不是投资推动⿎励说真话的⾃我修正机
制，⽽是研发出前所未有的错误增强机制，⿎励谎⾔与虚构。脸书在
2016年于缅甸推出的“即时⽂汇”（Instant Articles）项⽬，就是这种
错误增强机制之⼀。当时为了提升参与度，脸书在提供新闻频道奖励
时，只看频道的⽤户参与度（以点击次数与观看次数计算），⽽完全
不管所谓的“新闻”是否真实。2021年的⼀项研究发现，在该项⽬启动
前的2015年，缅甸脸书粉丝专⻚排名前⼗中有六个属于“合法媒体”。
到2017年，在“即时⽂汇”的影响下，“合法媒体”粉丝专⻚在排名前⼗
中只剩下两个。到2018年，前⼗⼤粉丝专⻚已经全部是假新闻与钓⻥
标题⽹站。
该研究的结论是，在脸书推出“即时⽂汇”项⽬之后，“⼀夜之间，缅
甸到处涌现出做钓⻥标题的⼈。这些⼈知道怎样制作引⼈上钩、愿意
参与的内容，他们每个⽉能赚⼏千美元的⼴告收⼊，相当于当地平均
⽉薪的10倍——直接由脸书⽀付”。由于脸书是当时缅甸最重要的⽹
络新闻来源，也就对缅甸的整体媒体环境造成了巨⼤影响，“在⼀个
脸书等于互联⽹的国家，质量低劣的内容淹没了其他信息来源”。脸
书与其他社交媒体平台并不是有意要让假新闻与令⼈惊骇愤慨的内容
充斥世界，但它们要求算法尽量提升⽤户参与度，最后就会造成这样
的后果。
回顾缅甸那场悲剧，佩因特·吞在2023年7⽉写邮件告诉我：“我曾经
天真地相信，社交媒体能够联结⼏⼗亿⼈的前额叶⽪质，以此提升⼈
类的意识，传播⼈类共同的观点。但我最后发现，社交媒体公司并没
有动⼒联结⼈们的前额叶⽪质，反⽽想联结边缘系统——这对⼈类⽽
⾔实在太危险了。”
