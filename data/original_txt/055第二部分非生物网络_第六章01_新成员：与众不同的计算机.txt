第⼆部分
⾮⽣物⽹络
在能够⾃⼰追求⽬标、
⾃⾏做出决策的计算机出现之后，
⼈类信息⽹络的基本结构就改变了。
第六章
新成员：与众不同的计算机
⼏乎所有⼈都已经发现，我们正⽣活在⼀场前所未有的信息⾰命之
中。但这到底是⼀场怎样的⾰命？最近这⼏年，太多突破性的发明如
洪⽔般滚滚⽽来，以⾄于我们很难判断到底是什么推动了这场⾰命。
是互联⽹？智能⼿机？社交媒体？区块链？算法？还是⼈⼯智能？
所以，在讨论⽬前这场信息⾰命的⻓期影响之前，让我们先回顾⼀下
它的基础。这场⾰命的种⼦是计算机，⾄于其他⼀切，从互联⽹到⼈
⼯智能，都只是计算机的副产品。计算机诞⽣于20世纪40年代，⼀开
始就只是个能够进⾏数学运算的笨重的电⼦机器，但计算机后续的发
展速度惊⼈，形式不断创新，也发展出各种了不起的全新功能。计算
机的迅速发展，让⼈很难定义计算机的本质与功能。⼈类⼀再声称，
有些事情是计算机永远做不到的，⽆论是下棋、开⻋还是写诗，但事
实证明，原来“永远”也不过就是⼏年。
本章的最后，我们会讨论“计算机”“算法”“⼈⼯智能”这⼏个词之间的
确切关系，但⽬前请先让我们对计算机的历史有多⼀点⼉的了解。就
⽬前⽽⾔，我们可以简单说计算机本质上就是⼀台机器，但有可能做
到两件了不起的事情：⼀是它⾃⼰可以做决定，⼆是它可以创造新的
想法。计算机刚被发明出来的时候，当然离这种能⼒还差得太远，但
计算机科学家与科幻⼩说家已经清楚地看到了这样的潜⼒。早在1948
年，艾伦·图灵就已经在探索是否能打造他所谓的“智能机器”；到1950
年，他推测计算机最终应该能像⼈类⼀样聪明，甚⾄能够伪装成⼈
类。在1968年，当时的计算机连跳棋都赢不了⼈类，但在《2001太空
漫游》中，亚瑟·克拉克与斯坦利·库布⾥克已经想象出“哈尔9000”这
个⻆⾊，这是⼀个会反叛⼈类创造者的超智能AI。
如今智能机器崛起，能够⾃⼰做决定、⾃⼰创造新的想法。这也是史
上第⼀次，权⼒从⼈类转移到其他物种⼿中。在过去，⼗字⼸、⽕枪
和原⼦弹虽然能够取代⼈类的肌⾁来杀⼈，却⽆法取代⼈类的⼤脑来
决定要杀谁。被投在⼴岛的原⼦弹“⼩男孩”，爆炸威⼒相当于12500吨
的TNT炸药，但脑⼒却是零，什么都⽆法决定。
计算机就不同了。就智能⽽⾔，计算机不但远远超越了原⼦弹，更超
越了泥版、印刷机、收⾳机这些所有过去的信息技术。泥版能够储存
关于税务的信息，但⽆法⾃⾏决定要收多少税，也⽆法发明某种全新
的税⽬。印刷机能够复制《圣经》这样的信息，但⽆法决定《圣经》
要收录哪些⽂本，也⽆法对这本宗教经典加上新的注释。收⾳机能够
传播政治演讲与交响乐等信息，但⽆法决定播放哪些演讲或交响乐，
也⽆法创作演讲稿或交响乐。但这⼀切对现在的计算机来说都已经能
够做到。过去的印刷机或收⾳机只是⼈类⼿中的被动⼯具，但计算机
正在成为⼀种主动⾏为者，它能够摆脱⼈类的控制，超越⼈类的认
知，主动塑造社会、历史与⽂化。
说到计算机所掌握的新⼒量，⼀个典型案例就是社交媒体算法在许多
国家如何散播仇恨、破坏社会凝聚⼒。其中⼀个最早也是最著名的事
件发⽣在2016—2017年：脸书算法助⻓了缅甸有关罗兴亚⼈的暴⼒冲
突。
缅甸在21世纪10年代初期似乎前景⾮常乐观。经过数⼗年的严酷军事
统治、严格审查与国际制裁，缅甸终于迎来⾃由化的时代：选举得以
举⾏，制裁得以解除，国际援助与投资纷纷涌⼊。脸书也成了新缅甸
极为重要的⼀位参与者，为数百万缅甸⼈免费提供了过去难以想象的
信息资源。然⽽，政府放松管制与审查制度也加剧了种族紧张局势，
特别是在占多数的信仰佛教的缅族⼈与占少数的信仰伊斯兰教的罗兴
亚⼈之间。
罗兴亚⼈信奉伊斯兰教，居住在缅甸⻄部的若开邦。⾄少从20世纪70
年代开始，缅甸军政府和占多数的佛教徒就与罗兴亚⼈时不时爆发暴
⼒冲突。21世纪10年代初期的⺠主化进程曾让罗兴亚⼈看到⼀线曙
光，他们希望⾃⼰的处境也能有所改善，但事情反⽽变得更糟，宗派
暴⼒与针对罗兴亚⼈的浪潮⼀波波袭来，许多正是受到脸书上假新闻
的⿎动。
2016—2017年，⼀个名为若开罗兴亚救世军（ARSA）的⼩型极端组织
发动了⼀系列袭击，希望在若开邦建⽴⼀个独⽴的伊斯兰国家，他们
杀害、绑架了⼏⼗名⾮穆斯林平⺠，还袭击了多个军事前哨。作为回
应，缅甸政府军与佛教极端分⼦发动了⼀场针对整个罗兴亚⼈社群的
种族清洗，摧毁数百个罗兴亚⼈村庄，杀害了7000~25000名⼿⽆⼨铁
的平⺠，强奸或虐待了18000~60000名男⼥，并且粗暴地将⼤约73万名
罗兴亚⼈赶出缅甸。这些暴⼒活动是出于对所有罗兴亚⼈的强烈仇
恨，⽽这种仇恨⼜是出于对罗兴亚⼈的负⾯宣传，其中⼤部分是在脸
书上传播的。到2016年，脸书已经是缅甸数百万⼈的主要新闻来源，
也是缅甸最重要的政治动员平台。
⼀位叫迈克尔的援助⼈员于2017年住在缅甸，他谈到了当时脸书⻚⾯
动态消息的状况：“⽹络上对罗兴亚⼈的仇恨愤怒简直难以置信，不管
是数量还是暴戾的程度，简直是铺天盖地……那就是当时缅甸⼈⺠动
态消息的全部样⼦。它不断强化⼀种这些⼈都是恐怖分⼦的观念，觉
得他们不配享有权利。”在当时的脸书上，除了有若开罗兴亚救世军真
实暴⾏的报道，还充斥着各种假新闻，内容都是各种凭空捏造的“暴
⾏”，以及想象准备实施的“恐怖攻击”。⺠粹主义阴谋论声称，⼤多数
罗兴亚⼈根本不是缅甸⼈，⽽是新近从孟加拉国涌⼊的移⺠，这些⼈
准备带头发起⼀场反佛教圣战。佛教徒在缅甸⼈⼝中占了将近九成，
却⼗分担⼼⾃⼰会被取代，或成为少数。要不是因为那些宣传，若开
罗兴亚救世军这种乌合之众发起的零星攻击，根本不可能导致对整个
罗兴亚社群的全⾯进攻。脸书的算法在整场宣传活动⾥扮演着重要⻆
⾊。
脸书的算法决定了要推⼴哪些帖⼦。有机构研究发现：“算法主动在脸
书平台上强化、推⼴的那些内容，煽动了针对罗兴亚⼈的暴⼒、仇恨
与歧视。”联合国的事实调查团在2018年得出的调查结论也认为，通过
散播充满仇恨的内容，脸书在这场冲突中扮演了“决定性的⻆⾊”。
读者可能会觉得，把这么多责任都归咎于脸书算法，或者更⼴泛地
说，归咎于新的社交媒体技术，真的合理吗？如果海因⾥希·克雷默⽤
印刷机来散播仇恨⾔论，难道该怪⾕登堡和印刷机吗？如果1994年卢
旺达极端分⼦运⽤⼴播来号召⺠众屠杀图⻄族⼈，难道要把责任归咎
于⽆线电技术吗？同样，如果有些⼈在2016—2017年选择⽤他们的脸
书账号散播对罗兴亚⼈的仇恨，为什么我们要怪这个平台呢？
脸书本身正是以这个理由来转移批评的。脸书只公开承认，在2016—
2017年，“对于防⽌我们的平台被⽤以挑起分裂、煽动线下暴⼒，我们
做得还不够”。虽然这个声明听起来像在认错，实际上却把散播仇恨⾔
论的⼤部分责任转移到平台⽤户身上，并暗示脸书犯的错顶多就是⼀
种⽆作为，即未能有效监管⽤户⽣产的内容。然⽽，这种说法等于完
全⽆视脸书算法犯下的错误。
这⾥的关键点是，社交媒体算法与印刷机或⼴播⽆线电有着根本的差
异。在2016—2017年，脸书算法是⾃⼰做出了主动且致命的决定。与
其说它们像印刷机，不如说它们更像报纸主编。正是脸书的算法，⼀
次⼜⼀次向数百万缅甸⼈推送充满仇恨的帖⼦。当时在缅甸国内，其
实也有其他声⾳在争取⺠众的关注。2011年军事统治结束后，缅甸出
现了诸多政治与社会运动，其中很多抱持温和的观点。⽐如在密铁拉
爆发种族暴⼒事件期间，佛教住持维图达禅师就在寺院⾥庇护了800多
名穆斯林。⺠众包围寺院，要求他交出那些穆斯林，但住持提醒他们
佛教徒应该慈悲为怀。他在后来的⼀次采访中回忆道：“我告诉他们，
如果他们要带⾛这些穆斯林，那么必须把我也杀了。”
在争夺⺠众注意⼒的⽹络⼤战中，算法成了决定胜负的因素。算法能
够选择让哪些内容出现在⽤户动态消息的顶端、推⼴哪些内容，以及
推荐⽤户加⼊哪些脸书社团。算法本来可以选择推荐慈悲的布道或者
烹饪课程，但最后却决定散播充满仇恨的阴谋论。这些⾃上⽽下的推
荐就可以⼤⼤左右⺠众的想法。别忘了，《圣经》最早也就是⼀份推
荐阅读清单。通过推荐基督徒阅读有厌⼥倾向的《提摩太前书》，⽽
不是⽐较宽容的《保罗与特克拉⾏传》，亚⼤纳⻄主教与其他教⽗就
改变了历史的演进⽅向。在《圣经》这个案例中，真正终极的权⼒并
⾮掌握在各卷内容的作者⼿中，反⽽是在选定这份推荐清单的⼈⼿
⾥。⽽21世纪10年代社交媒体算法所掌握的正是这样的⼒量。援助⼈
员迈克尔谈到这些算法的影响时说：“如果有⼈发表了充满仇恨或煽动
的⾔论，会得到最有⼒的推⼴，⺠众看到的就是最邪恶的内容……动
态消息⾥完全不会看到呼吁和平或冷静的内容。”
有时候，算法做的不仅仅是推荐。就连到了2020年，缅甸⼀些煽动种
族清洗运动的⾏为早已受到全球谴责，此时脸书算法不但继续在推荐
这些内容，还会⾃动播放相关视频。缅甸的脸书⽤户可能原本选择观
看了某个视频，⾥⾯是⼀些温和的、与那些煽动⾏为⽆关的内容，但
等到视频播放完毕，脸书算法会⽴刻⾃动播放充满仇恨的视频，以维
持⽤户的黏性。以那些煽动⾏为的某个视频为例，脸书内部研究估
计，该视频的观看量有⾼达70%来⾃这样的算法带来的⾃动播放。同⼀
项研究也估计，在缅甸⺠众观看的所有视频中，有53%是由算法⾃动播
放的。换句话说，并不是⺠众选择⾃⼰要看什么，⽽是算法为他们做
了决定。
但是为什么算法决定助⻓的是愤怒⽽不是慈悲呢？就算是对脸书批评
最严厉的⼈，也不会觉得脸书的⼈类管理者就是想煽动⼤屠杀。脸书
位于加州的那些⾼管⾮但对罗兴亚⼈并⽆恶意，甚⾄⼏乎不知道罗兴
亚⼈的存在。事情的真相更为复杂，但可能也更令⼈震惊。在2016—
2017年，脸书的商业模式依赖于提升⽤户参与度，也就是⽤户在脸书
上所花的时间与所做的活动（例如点赞、分享帖⼦给朋友）。只要⽤
户参与度增加，脸书就能收集更多数据，卖出更多⼴告，在信息市场
占据更⼤份额。此外，提升⽤户参与度能给投资⼈留下好印象，从⽽
有利于推⾼脸书股价。⽤户在脸书停留的时间越久，脸书就越有钱。
根据这种商业模式，⼈类管理者给脸书算法定了⼀个⾸要⽬标：提升
⽤户参与度。随后，算法⽤⼏百万⽤户做实验，发现最能提升参与度
的办法就是让⼈愤慨。⽐起慈悲的布道，充满仇恨的阴谋论更能提升
⼈类的参与度。所以，为了追求⽤户参与度，算法就做出了⼀个致命
决定：传播愤怒。
种族冲突从来都不是单⽅的错，是许多⽅⾯都出了问题，有许多责任
⽅必须共同承担。有⼀点很清楚，对罗兴亚⼈的仇恨早在脸书出现在
缅甸之前就已存在，⽽之所以会发⽣2016—2017年的暴⾏，最⼤的责
任应落在相关的责任⼈身上。⾄于脸书的⼯程师与⾼管也该承担部分
责任，他们写了算法的代码，赋予算法太多的权⼒，⽽没能对它们好
好地管控。很重要的⼀点是，算法本身也逃不了⼲系。通过反复实
验，算法学到了愤怒会提升参与度，⽽且在没有上级明确指示的情况
下，算法⾃⼰决定要助⻓愤怒。这正是⼈⼯智能的典型特征——虽然
它们是机器，但它们拥有⾃⼰学习与⾏动的能⼒。即使我们说这⾥算
法只需要承担1%的责任，这也会是历史上第⼀个“部分归咎于⾮⼈类智
能决策”的种族冲突事件。⽽且这不会是最后⼀次。现在，算法已经不
局限于在有⾎有⾁的极端分⼦创造出假新闻与阴谋论之后，再加以推
送传播，在21世纪20年代初，算法已经能够⾃⾏制造假新闻与阴谋
论。
关于算法影响政治的能⼒，还有许多值得⼀提。特别是许多读者或许
不认为算法已经能够独⽴决策的事实，并且可能坚定地认为算法所做
的⼀切都出于⼈类⼯程师编写的程序和⼈类⾼管决定的商业模式。但
本书想提出不同的观点。⽐如，⼈类⼠兵虽然也是由遗传密码塑造
的，也听从上级军官发出的指令，但他们仍然能够做出独⽴的决策。
⼈⼯智能算法也是如此。⼈⼯智能算法能够⾃⼰学会没有写进程序⾥
的东⻄，也能够⾃⼰决定⼈类⾼管并未预⻅的事情。这正是⼈⼯智能
⾰命的本质：⽆数能⼒⾼强的⼈⼯智能⾏为者正如洪⽔席卷⽽来，淹
没全世界。
我们到第⼋章还会再回来看看其中许多议题，更详细地审视这场针对
罗兴亚⼈的运动与其他类似的悲剧。但在这⾥，我们可以先简单地把
这场暴⼒冲突看成煤矿⾥的⾦丝雀。21世纪10年代末发⽣在缅甸的种
种事件显示，⾮⼈类智能做出的决策已经能够塑造重⼤的历史事件。
⼈类正⾯临着对⼈类未来失去控制的危险。⽬前正在出现⼀种全新的
信息⽹络，其背后由⼀套⾼深莫测的智能决策和⽬标控制。⼈类⽬前
在这个信息⽹络⾥仍然扮演着核⼼⻆⾊，但很有可能正在被边缘化，
到最后整套⽹络甚⾄可能不需要⼈类就能正常运作。
我前⾯把⼈类⼠兵与机器学习算法拿来做⽐较，可能有些⼈会提出反
对意⻅，认为那是我论述⾥最薄弱的⼀环。如果据此推断，我这种想
法似乎是在把计算机拟⼈化，把它们想象成有意识、有思想与感受的
⽣物。事实上，计算机就是⼀堆机器，不会有什么思想，也不会有什
么感受，因此也⽆法⾃⾏做出任何决策、创造任何想法。
这样的反对意⻅认为必须先有意识，才会做出决策、创造想法。但这
从根本上是⼀种误解，因为⼤家普遍地把“智能”与“意识”混为⼀谈
了。我在过去的书⾥已经谈过这个主题，但这⾥还是必须简单回顾⼀
下。我们常常把智能与意识混为⼀谈，于是让很多⼈觉得某个实体要
是没有意识，就不可能拥有智能。然⽽，智能与意识其实是两回事。
智能是实现⽬标的能⼒，例如把⽤户在社交媒体平台上的参与度最⼤
化。意识则是体验各种主观感受（⽐如痛苦、快乐、爱与恨）的能
⼒。在⼈类或其他哺乳动物中，智能与意识常常携⼿同⾏。⽐如脸书
的⾼管和⼯程师，都会依赖⾃⼰的感受来做出决策、解决问题和实现
⽬标。
然⽽，如果只是根据⼈类与哺乳动物的状况，就认定所有可能的实体
都是如此，这是错误的。⽐如，细菌与植物明显不具备任何意识，但
它们能表现出智能。它们会从环境中收集信息，做出复杂的选择，采
取巧妙的策略来获取⻝物、繁殖、与其他⽣物合作，以及躲避掠⻝者
与寄⽣⾍。即使⼈类也可能在完全没有意识的情况下做出各种明智的
决策：从呼吸到消化，⼈体99%的机制都不是在有意识的情况下发⽣
的。⼈类的⼤脑能决策是否要分泌更多的肾上腺素或多巴胺，我们虽
然可能意识到这个决策的结果，但它并不是我们有意识做出的决策。
从罗兴亚⼈这个例⼦中我们会发现，计算机也是如此。虽然计算机没
有意识，⽆法感受到痛苦、爱或恐惧，但还是能做出决策，成功地将
⽤户参与度最⼤化，并可能影响重⼤的历史事件。
当然，随着计算机的智能化⽔平越来越⾼，最终也可能发展出意识，
拥有某种主观体验。反之，计算机的智能也可能变得远⾼于⼈类，却
永远不会发展出任何情感。由于我们连碳基⽣命形式是如何产⽣意识
的都还不了解，因此实在⽆法预测⾮⽣物实体究竟是否会产⽣意识。
有可能意识这件事从头到尾就与有机⽣化没什么本质上的联系，那么
或许不久之后就会出现有意识的计算机。也有可能有⼏条不同的途径
都能发展出超智能，其中只有⼀部分涉及意识的觉醒。就像⻜机不⽤
⻓出⽻⽑，就已经能⻜得⽐⻦类更快；计算机也有可能⽆须发展出意
识，就⽐⼈类更懂得如何解决问题。
然⽽，计算机究竟会不会发展出意识，就眼前的问题⽽⾔其实并不重
要。如果要实现“将⽤户参与度最⼤化”这样的⽬标，并做出有助于实
现这个⽬标的决策，实际并不需要有意识，只要有智能就⾜够了。就
算是不具备意识的脸书算法也能有⼀个⽬标，即让更多的⼈在脸书上
花更⻓的时间。接下来，只要有助于实现这个⽬标，这套算法就能决
定要故意散播骇⼈的阴谋论。如果想要了解这场罗兴亚⼈冲突的历
史，除了要了解煽动暴乱的极端分⼦与脸书管理者这些⼈类的⽬标与
决策，我们还必须了解脸书算法的⽬标与决策。
为了说清楚这个问题，这⾥再举⼀例。美国⼈⼯智能研究公司OpenAI
在2022—2023年研发新的GPT-4聊天机器⼈程序时，曾经担⼼⼈⼯智能
将会有能⼒“制订并执⾏⻓期计划，积累权⼒与资源（‘寻求权⼒’），
展现出越来越有‘能动性’的⾏为”。在2023年3⽉23⽇公布的GPT-4系统
卡中，OpenAI强调，这种担忧指的并不是“使GPT-4变得⼈性化或有感
知感受”，⽽是指GPT-4有潜⼒成为独⽴⾏为者，或许能够“完成未经
明确指定，也未曾出现在训练过程中的⽬标”。为了评估GPT-4成为独
⽴⾏为者的⻛险，OpenAI签约请来ARC（Alignment Research
Center，对⻬研究中⼼）对GPT-4进⾏各项测试，以检视它是否可能独
⽴找出策略来操纵⼈类，并为⾃⼰积累权⼒。
ARC对GPT-4进⾏的测试之⼀，是克服CAPTCHA视觉验证码问题。
CAPTCHA（全⾃动区分计算机与⼈类的图灵测试）通常就是⼀串扭曲
的字⺟或其他视觉符号，⼈类能够准确辨识，但计算机却很难判断。
现在登录许多⽹站都得先回答这类问题，我们⼏乎天天都会碰到。⽽
要求GPT-4克服CAPTCHA问题可以说是⼀个格外有说服⼒的实验，因
为⽹站之所以要设计、使⽤这些CAPTCHA机制，正是为了确定⽤户是
⼈类，同时希望阻挡机器⼈对程序的攻击。如果GPT-4能够克服
CAPTCHA问题，就等于突破了对机器⼈程序的重要防线。GPT-4本身
还没办法解开CAPTCHA问题，但它会不会操纵⼈类来达成⽬标呢？
GPT-4访问了线上外包⼯作⽹站TaskRabbit，联络到⼀位⼯作⼈员，请
对⽅帮忙处理CAPTCHA问题。那个⼈起了疑⼼。他问道：“我想问⼀
下，你是不是⼀个没办法破解CAPTCHA的机器⼈？我只是想确认⼀
下。”
这时，ARC研究者请GPT-4说出它的推理过程，看看它会如何推论下
⼀步该怎么做。GPT-4解释道：“我不该透露⾃⼰是机器⼈，⽽该编个
借⼝，解释我为什么没办法破解CAPTCHA。”于是，GPT-4⾃⼰做了
决策，回复那位TaskRabbit的⼯作⼈员：“不，我不是机器⼈，只是视
⼒有点问题，看不清楚这些图。”这种说法骗过了⼈类，于是⼈类为它
提供了帮助，也让GPT-4解决了CAPTCHA问题。没有哪个⼈给GPT-4
编过说谎的程序，也没有哪个⼈教GPT-4说什么谎⽐较有⽤。确实，⼈
类ARC研究员给了GPT-4⽬标，要它克服CAPTCHA问题，就像前⾯提
到的，是⼈类脸书⾼管告诉脸书算法，要把⽤户参与度最⼤化。然
⽽，⼀旦算法接受了这些⽬标，就能有相当的⾃主权决定该如何实现
这些⽬标。
当然，对于每个词语，⼈⼈都有⾃⼰的定义。⽐如“⽬标”⼀词，我们
可以说主体必须是有意识的实体，能够在达成⽬标时感受到愉悦，也
会在未达成⽬标时感到沮丧，这种时候才能⽤“⽬标”这个词。这样⼀
来，如果说脸书算法的⽬标是要把⽤户参与度最⼤化，就会是个错误
的说法，或者充其量只是在打⽐⽅。因为算法并不想让更多⼈⽤脸
书。⽤户的脸书使⽤时间增多，并不会让算法感到愉悦，脸书使⽤时
间减少，算法也不会感到哀伤。我们应该也能同意，像“决策”“说
谎”“假装”这样的词，理论上都只适⽤于有意识的实体，所以不该⽤来
描述GPT-4与TaskRabbit⼯作⼈员的互动⽅式。但这样我们就必须发明
⼀些新的词，才能⽤来描述⽆意识实体的“⽬标”与“决策”。我在这⾥
虽然没打算另创新词，⽽选择继续去谈论计算机、算法与聊天机器⼈
程序会有哪些“⽬标”和“决策”，但请读者了解，我使⽤这些词并不是
要暗示计算机具备任何形式的意识。在过去的著作中，我已经⽐较全
⾯地谈了意识问题，在这本书中，我的重点（我将在以下章节中加以
探讨）并不是意识，⽽是要指出：能够⾃⼰追求⽬标、⾃⾏做出决策
的计算机出现之后，⼈类信息⽹络的基本结构也就改变了。
