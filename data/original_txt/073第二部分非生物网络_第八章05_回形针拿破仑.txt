回形针拿破仑
在计算机⽹络的背景下，⼀致性问题的危险特别⾼，原因之⼀就是这
个⽹络的权⼒可能远远⾼于过去任何⼈类官僚机构。如果是超级智能
计算机出现了⽬标与⼈类不⼀致的状况，就可能导致前所未有的灾
难。哲学家尼克·波斯特洛姆2014年的著作《超级智能》中就有⼀个思
想实验，试图说明这种危险。这本书类似歌德的《魔法师学徒》。波
斯特洛姆要我们想象⼀下，有⼀家回形针⼯⼚买了⼀台超级智能计算
机，⼯⼚的⼈类主管要它完成⼀项看来再简单不过的任务：⽣产回形
针，越多越好！结果为了完成这个⽬标，这台回形针计算机征服了整
个地球，杀死了所有⼈类，并派出远征队占领更多⾏星，再⽤取得的
丰富资源在整个银河系设⽴⽆数个回形针⼯⼚。
这个思想实验的重点在于计算机只是乖乖地做⼈类要它做的事（像歌
德笔下的那把魔法扫帚）。它先“意识”到，要建造更多⼯⼚、⽣产更
多回形针，就需要电⼒、钢铁、⼟地与其他资源；接着⼜“意识”到，
⼈类不太可能会放弃这些资源，于是这台超级智能计算机为了⼀⼼追
求这个既定⽬标，在过程中直接消灭了所有⼈类。波斯特洛姆想强调
的是，计算机的问题并不在于它们特别邪恶，⽽在于它们特别强⼤。
⽽计算机越强⼤，我们就越要⼩⼼为其确⽴⽬标，务必让计算机与⼈
类的终极⽬标完全⼀致。如果只是个⼝袋计算器，即使我们给它确定
的⽬标与⼈类的很不⼀致，后果也微乎其微。但如果是超级智能计算
机，如果确定了⼀个与⼈类利益极不⼀致的⽬标，就可能催⽣⼀个反
乌托邦。
这个回形针思想实验或许听起来很离谱，似乎与现实完全脱节，但在
波斯特洛姆于2014年提出这个想法的时候，要是那些硅⾕⾼管注意了
这个想法，他们或许就不会那么莽撞地要求算法尽量提升⽤户参与
度。脸书与YouTube的算法的表现，与波斯特洛姆想象的算法⼀模⼀
样。当被要求尽量⽣产回形针，⽽且产量越多越好的时候，算法就会
想要把整个宇宙的物质都变成回形针，就算摧毁⼈类⽂明也在所不
惜。当被要求尽量提⾼⽤户参与度，⽽且数字越⾼越好的时候，脸书
与YouTube的算法就想要把整个“社群宇宙”都变成⽤户参与度，即使
会破坏缅甸、巴⻄与许多其他国家的社会结构，也在所不惜。
在计算机这个领域，波斯特洛姆的思想实验还能让我们看到⼀致性问
题格外重要的第⼆个原因：因为都是⾮⽣物实体，所以它们采⽤的策
略很可能是所有⼈类从未想到的，⾃然⽆⼒预⻅并阻⽌。这⾥有⼀个
例⼦：2016年，达⾥奥·阿莫迪设计了⼀个名为“宇宙”（Universe）的
项⽬，想要研发⼀款通⽤⼈⼯智能，它知道怎样玩⼏百种不同的计算
机游戏。这款⼈⼯智能在各种赛⻋游戏中表现出⾊，于是阿莫迪决定
让它试试赛船游戏。但不知道为什么，这款⼈⼯智能直接把船开进⼀
个港⼝，接着就只是不断绕圈进进出出。
阿莫迪花了很久才搞清楚问题出在哪⾥。之所以会出现这个问题，是
因为阿莫迪⼀开始不知道怎样告诉这款⼈⼯智能，它的⽬标是赢得⽐
赛。对算法来说，“赢”并不是⼀个明确的概念。如果把“赢得⽐赛”翻
译成计算机能懂的语⾔，阿莫迪就必须把排位、与其他参赛船只的相
对位置之类的复杂概念都⽤形式语⾔表达出来。于是阿莫迪决定换个
简单的办法，告诉⼈⼯智能得分越⾼越好。在他看来，得到最⾼分的
应该和赢得⽐赛差不多，毕竟之前的赛⻋游戏都是这样的。
然⽽，那款赛船游戏有⼀个赛⻋游戏没有的特点，⽽聪明的⼈⼯智能
找到了游戏规则的漏洞。在这款赛船游戏中，如果领先其他船只，玩
家就能得到⼤量积分（这和赛⻋相同），然⽽船只每次靠港补充能量
的时候，也能得到⼀些积分。结果⼈⼯智能就发现，与其试着超越其
他船只，不如不断绕圈进出港⼝，这样反⽽能够更快积累更多积分。
显然，⽆论是游戏的⼈类开发者还是阿莫迪都没注意这个漏洞。⽽这
款⼈⼯智能所做的，正是这款游戏⿎励它做的事，只是这并不是⼈类
想看到的。这正是⼀致性问题的本质：我们奖励的是A⾏为，却希望
得到B结果。要是我们希望计算机能带来最⼤的社会效益，就不该因
为它们带来了最⾼的⽤户参与度⽽给予奖励。
担⼼计算机⼀致性问题的第三个原因，在于计算机与⼈类实在差异太
⼤，所以就算我们不⼩⼼给出了与⼈类利益不⼀致的⽬标，计算机也
不太会有所警觉或要求说明。如果那个赛船⼈⼯智能是⼈类玩家，应
该就会意识到，利⽤⾃⼰在游戏规则⾥发现的漏洞⼤概不能真正赢得
⽐赛；如果那个回形针⼈⼯智能是⼈类官僚，应该就会意识到，⼈类
的初衷⼤概不是让它为了⽣产回形针⽽毁灭⼈类。但正因为计算机不
是⼈类，也就不能单纯信赖计算机会找出并警告可能的利益不⼀致现
象。在21世纪10年代，YouTube与脸书的管理团队早就受到⼈类员⼯
（与外部观察者）的连番轰炸，警告算法正在造成的危害，但算法本
身从头到尾浑然未觉。
随着我们让算法在医疗保健、教育、执法和许多其他领域拥有越来越
⼤的权⼒，⼀致性问题也变得越来越严重。要是不设法解决，问题绝
不只是算法让赛船⼀直绕圈刷积分⽽已。