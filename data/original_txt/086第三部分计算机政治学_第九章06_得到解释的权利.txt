得到解释的权利
计算机正在做出越来越多关于我们的决定，有些只关乎⽇常⼩事，但
有些关乎⽣命⼤事。除了⽤来量刑，算法在我们能不能上⼤学、能不
能找到⼯作、能不能得到各项福利，以及申请贷款是否成功等⽅⾯也
发挥着越来越⼤的影响。同样，算法也会影响我们会得到怎样的医
疗、得⽀付多少保费、会听到怎样的新闻，以及会与谁约会。
随着社会把越来越多的决定权交给计算机，⺠主的⾃我修正机制、透
明度与问责制都会受到挑战。如果算法如此⾼深莫测，⺠选官员如何
被监督？所以，已经有越来越多的⼈要求保障⼀项新的⼈权：得到解
释的权利。2018年⽣效的欧盟《通⽤数据保护条例》规定，如果算法
做出对某⼈的决定（例如拒绝提供信贷），当事⼈有权得到相关解
释，也能在由⼈类组成的某个机构或个⼈⾯前挑战这项决定。理想情
况下，这应该能够制衡算法的偏差，也能让⺠主的⾃我修正机制得以
找出并修正⾄少部分计算机的重⼤错误。
但这种权利究竟能否落实？穆斯塔法·苏莱曼是这个领域的世界级专
家，是⼈⼯智能公司DeepMind（可以说是全球数⼀数⼆的⼈⼯智能
企业）的联合创始⼈和前⾸席执⾏官，他过去的成就包括研发出
AlphaGo（阿尔法围棋）程序等。AlphaGo专为下围棋⽽设计，在这
种策略性棋盘游戏中，两名玩家通过吃⼦围地来击败对⽅。这种游戏
发明于古代中国，远⽐国际象棋复杂。因此，就算计算机击败了⼈类
的国际象棋世界冠军，专家依然相信计算机下围棋永远⽆法赢过⼈
类。
正因如此，AlphaGo在2016年3⽉击败韩国围棋冠军李世⽯的时候，围
棋界和计算机⾏业的专家都⽬瞪⼝呆。苏莱曼在2023年的著作《即将
到来的浪潮》（The Coming Wave）中谈到了这类⽐赛最重要的时刻
——这个时刻重新定义了⼈⼯智能，许多学界与政界也认为这是⼈类
历史的转折点。2016年3⽉10⽇，⽐赛进⼊第⼆局。
“接着……第37⼿，”苏莱曼写道：“这⼀⼿完全说不通。AlphaGo显然
被吓坏了，盲⽬⽤了显然必败的策略，任何职业棋⼿都不会这么下。
现场直播的两位解说员都是排名顶尖的专业棋⼿，他们也认为这是‘很
奇特的⼀⼿’，并认为这⼀⼿是‘⼀个错误’。这⼀⼿奇特到李世⽯⾜⾜
花了15分钟才做出回应，其间甚⾄得先离开棋局到外⾯⾛⼀⾛。我们
在监控室看着，⽓氛紧张到⾮常不真实。但随着终局逼近，当初‘错
误’的⼀⼿被证明⾄关重要。AlphaGo再度胜出。围棋的策略就这样在
我们眼前被改写。我们的⼈⼯智能，找出了⼏千年来最杰出的棋⼿都
没想到的棋步。”
第37⼿成为⼈⼯智能⾰命的象征，原因有⼆。第⼀，这让⼈看到⼈⼯
智能本质上的⾮⼈类与难以理解。在东亚，围棋绝不只是⼀种游戏，
⽽且是⼀种珍贵的⽂化传统。⾃古以来，合称“四艺”的琴、棋、书、
画，是⽂⼈雅⼠必须熟习的技艺。2500年来，下过围棋的⼈不计其
数，形成了各种思想流派，各有不同的策略与哲学。但过了⼏千年，
⼈类⼼智还是只探索了围棋领域的部分区域，⾄于其他区域，⼈类连
想都没想过，这些区域就⼀直是⽆⼈之境。但⼈⼯智能并不受⼈类⼼
智的限制，于是得以发现并探索那些⼈所未⻅的区域。
第⼆，第37⼿展示了⼈⼯智能的⾼深莫测。就算AlphaGo下了这⼿⽽
赢得胜利，苏莱曼与团队也⽆法解释AlphaGo到底是怎么决定要下这
⼀⼿的。就算法院命令DeepMind向李世⽯提供解释，这项命令也没
有⼈能够执⾏。苏莱曼写道：“我们⼈类⾯临⼀个全新的挑战——未
来的新发明，会不会完全超越我们理解的范围？以前，就算要补充⼤
量的细节，创作者也能够解释某件事物是如何运作的，背后有什么原
理。但现在，情况不是这样了。许多科技与系统已经变得如此复杂，
没有任何⼀个⼈有能⼒真正理解……在⼈⼯智能领域，那些正在⾛向
⾃主的信息⽹络⽬前就是⽆法解释。你没有办法带着⼈⼀步⼀步⾛过
整个决策过程，准确解释为什么算法会做出某项特定预测。⼯程师没
办法看到机器的外壳下⾯发⽣了什么，更没办法轻松⽽详尽地解释各
种事情是如何发⽣的。GPT-4、AlphaGo等技术软件是⼀个⼜⼀个⿊
盒⼦，输出的信息与做出的决定，就是基于各种不透明⽽⼜极其复杂
的微⼩信号链。”
这种⾮⼈类的、令⼈难以理解的智能会让⺠主受到损害。如果越来越
多关于⼈⺠⽣活的决定都是在⿊盒⼦⾥完成的，选⺠⽆法理解，也⽆
法挑战那些决定，⺠主就会停摆。特别是如果这些由⾼深莫测的算法
做出的关键决定不仅影响个⼈⽣活，甚⾄涉及美联储利率这样的集体
事务，世界会变成什么模样？⼈类公⺠或许还是会继续投票选出⼈类
的领导⼈，但这不就是仪式⽽已吗？时⾄今⽇，只有很少⼀部分⼈真
的了解⾦融体系是如何运作的。经济合作与发展组织2016年的⼀项调
查发现，⼤多数⼈甚⾄连复利这种简单的⾦融概念都⽆法理解。英国
国会议员肩负重任，要监督全球最重要的⾦融中⼼的运⾏，但2014年
的⼀项针对英国国会议员的调查发现，只有12%的议员真正了解银⾏
放贷的过程会创造新的货币。⽽这件事只是现代⾦融体系最基本的原
理之⼀。正如2007—2008年的全球⾦融危机让我们看到的，⼀些更复
杂的⾦融⼯具（例如担保债务凭证）与其背后的原理，只有极少数⾦
融专家才能理解。等到⼈⼯智能创造出更复杂的⾦融⼯具，全球再也
没有任何⼀个⼈类真正了解⾦融体系时，⺠主会发⽣什么改变？
近期之所以会出现⼀波⺠粹政党与魅⼒领袖的浪潮，原因之⼀就在于
我们的信息⽹络变得越来越⾼深莫测。信息仿佛排⼭倒海⽽来，令⼈
难以消化、不知所措，⺠众觉得⾃⼰⼀旦再也看不懂世界是怎么回
事，就很容易成为阴谋论的猎物，于是想向某个⾃⼰能够理解的事
物，也就是某个⼈类，寻求救赎。遗憾的是，在算法逐渐主导世界的
现在，虽然魅⼒领袖绝对都有其⻓处，但⼀个⼈不论多么⿎舞⼈⼼或
才华横溢，单凭⼀⼰之⼒绝不可能破解算法运作的谜题，也⽆法确保
算法真正公平。问题在于，算法在做出决定时参考了⼤量的数据，但
⼈类却很难有意识地对⼤量数据进⾏反思、做出权衡。我们就是⽐较
喜欢⾯对单⼀的数据。如果碰上复杂的问题（不管是贷款、疫情还是
战争），我们常常希望能找出某个单⼀的理由，采取特定的⾏动。这
就是所谓的单⼀归因谬误。
⼈类不善于同时权衡诸多不同因素，所以如果有⼈为某个决定给出许
多理由，反⽽会让⼈觉得可疑。假设有位好朋友没来参加我们的婚
礼。如果他只讲了⼀个理由（“我妈住院了，我得去看她”），听起来
似乎很合理。但如果他列了50个理由呢？“我妈有点⼉不舒服，我这
个礼拜得带狗去看兽医，我⼿上有⼯作，当时还在下⾬……我知道每
个理由听起来都不算是没去的合理理由，可是这50个理由加在⼀起，
我就没办法去参加你的婚礼了。”我们之所以不会说出这样的话，是
因为我们的脑⼦不会这样想。我们不会有意识地在⼼⾥列出50个不同
的理由，分别给予不同权重，再全部加总得出结果。
然⽽，算法正是这样评估我们的犯罪⻛险或信⽤⽔平的。以COMPAS
为例，它的犯罪⻛险评估基于⼀份有137个项⽬的问卷。那些拒绝发放
贷款的银⾏使⽤的算法也是如此。要是欧盟的《通⽤数据保护条例》
要求银⾏解释算法究竟是如何做出决定的，这⾥的解释绝不会只有⼀
句话，⽽很有可能包括⻓达⼏百甚⾄⼏千⻚的数字与⽅程式。
想象⼀下，银⾏的解释信⼤概会这样写：“敝⾏的算法采⽤⼀套精确
的积分系统来评估所有贷款申请，共考虑1000个不同类型的因素，并
将所有因素的分值相加得出总分。总分为负数，则属于低信⽤度客
户，贷款⻛险过⾼。贵客户总分为-378分，因此请恕敝⾏⽆法核发贷
款。”接着，信⾥可能会详细列出这套算法所考量的1000个相关因
素，有些甚⾄⼤多数⼈会觉得根本⽆关，例如⼏点提出的申请、申请
⼈⽤的是哪款智能⼿机。接着再到这封信的第601⻚，银⾏可能会解释
说：“贵客户通过智能⼿机提出申请，⽽且是苹果⼿机最新的机型。
根据分析数百万份过去的贷款申请，敝⾏的算法发现⼀个规律——使
⽤苹果⼿机最新机型的申请者，还款的可能性⾼0.08%。因此，算法
已为该客户的总分加了8分。然⽽，贵客户在申请时，⼿机电量已降⾄
17%。根据分析数百万份过去的贷款申请，敝⾏算法发现另⼀个规律
——允许智能⼿机电量低于25%的客户，还款可能性会降低0.5%。因
此，算法已为该客户的总分扣50分。”
你可能觉得银⾏太莫名其妙了，并抱怨说：“光是因为我的⼿机电量
低，就拒绝核发贷款，这合理吗？”但这种说法实在是误会。银⾏会
解释：“电量并不是唯⼀的因素，那只是敝⾏算法考虑的1000个因素
⾥⾯的⼀个⽽已。”
“可是你们的算法难道没看到，我在过去10年⾥只透⽀过两次？”
“算法显然注意到了这⼀点，”银⾏可能会这样回答：“请看第453⻚。
您在这⾥得到了300分。是其他所有因素的作⽤，才让您最后的总分
为-378分。”
这种做决定的⽅式，虽然对我们来说可能很陌⽣，但显然也有些潜在
的优势。⼀般来说，做决定的时候能考虑到所有相关的因素，⽽不是
只看⼀两项⽐较突出的事实，通常都是好事。当然，究竟哪些信息才
算“相关”，还有很⼤的争论空间。核发贷款的时候，由谁决定智能⼿
机型号或申请⼈肤⾊这些信息与贷款申请是否相关？然⽽，不论我们
如何定义相关性，“能够考虑更多因素”应该都是好事。事实上，许多
⼈类的偏⻅正是因为只专注于⼀两个因素（例如肤⾊、性别或是否残
疾），⽽忽略了其他信息。银⾏与其他机构之所以越来越喜欢⽤算法
做决策，正是因为算法能够⽐⼈类将更多因素纳⼊考量。
但到了要给出解释的时候，就会出现可能难以克服的障碍。对于⼀项
参考了这么多因素所做出的决定，⼈类的⼼智要怎么加以评估分析？
我们很可能会觉得，威斯康星州最⾼法院实在应该要求Northpointe公
司披露COMPAS判定埃⾥克·卢⽶斯具有⾼犯罪⻛险的过程细节。但
就算真的披露了所有资料，卢⽶斯和法官⼜真的能理解吗？
这⾥的问题还不只是有⼤量的因素需要考量，或许最重要的是，我们
⽆法理解算法是怎么从数据⾥找出规律模式并决定如何配分的。就算
我们知道银⾏算法会找出允许智能⼿机电量低于25%的⼈，并给这些
⼈扣掉⼀定的分数，我们要怎么判断这公不公平？毕竟并不是⼈类⼯
程师写了这条规则要求算法照办，⽽是算法分析了过去⼏百万份贷款
申请，从中发现规律模式，才得出了这样的结论。难道光凭贷款申请
⼈⾃⼰就能去检查所有资料，判断这种规律模式是否真的公正可靠
吗？
然⽽，在这⽚数字乌云的背后，其实还有⼀⽚灿烂的阳光。虽然⼀般
⼈确实⽆法独⼒去检视那些复杂的算法，但专家团队在⼈⼯智能⼯具
的协助下，评估算法的决策是否公平，其结果可能⽐⼈类评估⼈类的
决策是否公平来得更可靠。毕竟，虽然⼈类的决策表⾯上基于我们意
识到的那⼏个数据点，但是⼈类的潜意识会受到成千上万其他数据点
的影响。虽然每个决定都是⼤脑⾥⼏⼗亿个神经元互动之后的结果，
但由于我们并未意识到那些潜意识，因此⼀旦去回想或解释，我们就
常常只会从单⼀数据点来加以说明。这样⼀来，要是有⼀位⼈类法官
判处我们6年有期徒刑，我们（或法官）要怎样才能确定这项判决真的
是出于公正的考量，⽽没有受到半点潜意识中的种族歧视，甚⾄是法
官当时肚⼦饿的影响？
对有⾎有⾁的法官来说，⾄少以⽬前的⽣物学知识，这个问题是⽆解
的。相较之下，如果是由算法来做出决定，理论上我们还是能够得知
其中所有的考量因素，以及每个因素被赋予了多⼤的权重。从美国司
法部到⾮营利新闻机构ProPublica的多个专家团队，都对COMPAS进
⾏了仔细的拆解分析，想了解其中究竟可能有怎样的偏差。这些团队
不但能发挥众⼈合作之⼒，甚⾄还能利⽤计算机的⼒量。就像是做贼
的往往最知道怎么去抓贼，所以我们也⼤可⽤算法来审查算法。
这⼜会带出⼀个问题：要怎样才能确定那个负责审查的算法本身没有
⽑病？这是个先有蛋还是先有鸡的问题，到头来并不会有⼀个单纯的
技术上的解决⽅案。不论研发了怎样的技术，⼈类都必须维持官僚机
构制度，由⼈类负责审核算法，决定要不要盖下那个许可的印章。这
些机构制度将结合⼈类与计算机的⼒量，确保新的算法系统安全公
正。要是没有这样的机构制度，就算我们通过了让⼈类有权得到解释
的法规，甚⾄施⾏了禁⽌计算机偏差的规定，⼜有谁能够真正加以执
⾏？
